Apachi Kafka is a popular open- Source distributed streaming platform it seamlessly handles real-time data streaming and processing ensuring that information flows smoothly and reliably Kafka is instrumental in even driven architecture it serves as a messaging leer in microservices promoting a scalable and Loosely coupled architecture in this video we'll explore the diverse applications of Kafka a widely used opsource streaming processing platform that has revolutionized the way Enterprise handle and process data through through the video we'll delve into the integrate details of Kafka architecture exploring its key components and their roles in the overall data processing pipeline we shed lights on topics partitions producers consumers consumer groups Kafka clusters and Brokers explaining how they work together to ensure efficient and reliable data storage and retrieval one of the key highlights of the video is the demonstration of kafra capabilities in handling high volume of data streams we'll walk through practical examples where Kafka inest and processes millions of events per second illustrating the scalability and resilience by the end of this video you will have a solid understanding of all the basics of Kafka so let's get [Music] started companies often have a source system such as a database which act as a central repository for valuable data this data is crucial for various business processes and decision- making however as organizations grow and evolve the need arises to share and utilize this data across different departments and systems this is where the concept of data migration comes into play data migration involves transferring data from The Source system that is the system where the data originates to a Target system or systems where the data is consumed the purpose of data migration can be diverse such as consolidating data from multiple sources upgrading to a newer systems or integrating with external systems for a seamless data Exchange initially data migration may seem like a straightforward process it may involve writing code to extract data from The Source system transform it to fit the requirements of the target system and finally load it into a Target system but as your business grows and evolves you may find yourself with multiple Source systems and multiple Target systems this can significantly complicate data integration challenges as each Source system must communicate with each Target systems to share information so previously if you had four sour systems and four Target systems you would need to create 16 integration points to make everything work each integration involves several complexities such as protocol differences that is data can be transmitted over TCP HTTP rest FTP jdbc Etc or the data format consern that is the data itself can be binary or CSV Json a or protool Etc additionally each Source system experiences increased load due to the numerous connections and requests for data extraction so basically data integration presents various challenges when dealing with multiple source and Target systems ensuring data consistency maintaining data quality and protecting data security and achieving efficiency and scalability are key challenges Apachi kafa addresses some of these issues by offering a scalable reliable and real-time data streaming platform that addresses these challenges headon Apache kaf covers initially conceived by LinkedIn as an open source project and is currently primarily maintained by a Consortium of notable corporations including confluent IBM claa and Linkedin itself kafka's robust distributed architecture coupled with the fall tolerance allows for system upgrades and maintenance without disrupting operations to understand and appreciate the power of Kafka we first need to understand the basics of messaging which I have explained in my previous videos in detail where I have talked in depth about peer-to-peer and pops up patterns explaining their limitations advantages and uses with examples in a sense message involves sending a message from one location to another it has three components a producer which is the entity responsible for creating and sending messages to one or more cues a queue which is a buffer data structure that receive messages from producers and delivers them to consumers in a first in first out manner once a message is delived it is permanently removed from the queue eliminating the possibility of retrieving it later the consumer is the entity subscribed to one or more cues receiving messages when they are published now that we know how messaging works let's dive into Kafka world at the core of Apache Kafka lies a publ subscribe architecture producers are data Publishers they publish data to designated topics while consumers are data subscribers who eagerly sign up for those same topics this recoupling creates a highly scalable and manageable environment for data integration pipelines let's break down these fundamentals starting with the concept of topics partitions and offsets Kafka operates within a cluster think of Kafka cluster as a group of computers or servers working together as a single system this cluster is what provides the power and reliability to handle massive amounts of data flowing through your topics Kafka topics are designed to handle continuous streams of incoming data it's like a river of information constantly flowing through your Kafka system each topic represents a specific category of this data flow imagine you are running a hot new social media app people are constantly posting updates and you need a system to handle this constant data flow you could have a post topic for all those amazing user updates a likes topic for every time someone shows some love and maybe even a follows topic to track the connections between users just like different hashtags categorize post on social media topics categorize your data streams in Kafka think of topics like tables in a database but without the rigid constraints unlike traditional databases with strict data types and schemas Kafka topics are much more flexible you can send any type of data you want to a Kafka topic you can throw in a Json data text images anything this makes Kafka well suited for handling the varied data formats common in modern applications Kafka isn't meant for permanent storage data is kept for certain duration usually a week by default and then it is automatically cleaned up this ensures Kafka focuses on processing realtime data rather being a historical archive Kafka topics are also immutable which means that once data is written it becomes a permanent record you can't go back and edit or delete individual messages and this is different from a database where you can update rules understanding immutability and data retention is crucial for Designing systems on top of gafka since data can't be changed Kafka encourages even driven architecture that is systems that react to events as they happen think realtime analytics or triggering notifications based on user actions I will talk more on these Concepts in my future video so stay tuned now these topics can get pretty busy to handle the high volume Kafka slices each topic into smaller chunks called partitions imagine each topic as a highway with multiple Lanes these partitions are those Lanes allowing for faster processing of the data stream you can even have as many partitions as needed depending on the amount of data flowing through your topic each message is assigned to a specific partition messages within a partitions are ordered meaning they will be processed in the order they will received the sequence of messages in a topic is called a data stream and this is why Kafka is called a data streaming platform every single message that zips through a partition gets a unique ID called an offset it is like a serial number assigned to each message in the order it arrives this keeps things organized within each partition think of it like a queue the first message gets offset zero the second message gets offset one and so on and so forth it is a unique ever increasing number this way you can always track the exact position of a message within the partition if something crashes you can restart reading from the last process message avoiding duplicates or mis data so offsets ensure messages within a partitions are processed in the exact order they arriv without them it would be a chaos so when a user post something the update gets published to the post topic Kafka might automatically distribute the messages across one of the partitions within the post topic within that partition the messages gets assigned an offset based on its arrival order now other parts of your system acting as consumers can read and process these messages maybe one consumer analyzes the sentiment of post while another pushes them into the newsfeed the key points to remember are once data is written to Kafka it's like writing an ink you can't change or delete it Kafka keeps your data for a specific period before deleting it messages within imp partitions are processed in the order they arrived based on their offsets but there is no guaranteed order across different partitions and finally by default messages are placed into partitions randomly for load balancing however you can also specify a key to ensure related messages end up in the same partition now say our social media app is buzzing with post likes and new connections kka producers take the center stage here producers are the parts of your app that create and publish these messages for every person posting an awesome photo there's a producer saying hey Kafka store this photo for every like button clicked there's a producer sending a small message into the likes topic producers don't just send data randomly they know which topic it belongs to and even which partition within that topic just like you decide where your social media content goes General feed or a specific hashtag based topic producers make similar calls sometimes a message will have a special key attached maybe it's the user ID or a hashtag for the post this key guarantees that all messages within the same key end up in the same partition within the topic this is how we maintained order for specific things like making sure one users post end up in the right if there is no key the message gets set out round robin style between partitions this helps in load balancing the workload across your Kafka cluster so no single partition gets overwhelmed for example when you're posting in your social media app let's say user post a hilarious Meme here is what the producer might do the producer bundles the meme image some text and the user ID into a Kafka message the user ID acts as the key the producer then picks up the post topic and because we have a key it figures out the correct partition based on the user's ID and finally the message zooms off to the right partition and gets assigned its offset but there is one crucial step before sending serialization Kafka deals in bytes the raw building blocks of computer data your messages though might be complex objects like user profiles or image data serialization translates this object into a bite format Kafka can understand and store during message construction when the producer creates a message containing the mem image data a caption and the user ID here is where the chosen serializer comes in maybe we are using a Json serializer this takes the entire message which is the meme image data caption and the user ID and transforms it into a stream of bytes essentially a Json representation in by format so now the producer has a message Kafka can handle and it sends the serialized message to the the appropriate partition in the post topic cavka comes with several built-in serializers for common data formats like spring integers and Json you can even plug in custom serializers if you have unique data types in your app now producers are the parts of your app that create and publish this messages we need another side to complete the picture that is where kafa consumers come in consumers are the parts of your app that read and process the data from Kafka topics so for every like every comment every new post post you might have consumers doing different jobs consumers subscribe to specific topics they are interested in maybe one consumer analyzes post for trending hashtags while another sends out this cute your friend like this notification consumers also read and process and here is where the magic happens consumer fetch messages from their assigned partition in the order they arrived remember those handy offsets but wait Kafka deals in bytes and our app needs usable data this is where deserialization comes in the consumer uses a der serializer this is the flip side of the serializer we saw with the producers to turn those bytes back into human readable form like Json or PL text imagine this process as unwrapping the message from Kafka and once TC realized the consumer can get to work on the actual data updating a real-time Trend chart triggering notifications or anything else your application needs consumers track their own progress Within part itions this way if there is a hiccup and a consumer needs to restart they know exactly where they left off and can avoid missing on reprocessing data often you'll have multiple instances of consumers working together to handle the load this is a Kafka consumer group it's like having a team tackle a giant news feed within a consumer group each consumer gets assigned a chunk of the partition from a topic and this helps to balance the workload and handle a high volume of data if one consumer in the group goes down its work is seamlessly reassigned to others this ensures your data is always being processed without missing a beat while you can have several independent consumer instances to handle the Giant news feed consumer groups offer several key advantages without consumer groups if you have multiple instances reading from the same topic they'll all receive and process the same messages this leads to duplicate work and potential issues for example imagine multiple instances trying to send the news to a user within a consumer a group each partition gets assigned to only one consumer instance this guarantees that every message in a partition is processed by exactly one member of the group so when a consumer group contains just one consumer it will get all messages from all partitions in the Kafka topic when a consumer group contains two consumers each consumer in the group will be assigned to half the partitions in the topic the consumer group will continue to balance consumers to partitions until the number of consumers is the same as partitions once there are more consumers than partitions the excess consumers will sit idle and receive no messages this means the number of partitions in the topic will be the limit on the effective number of consumers that consume messages at any given time you can also have multiple consumer groups reading from the same topic each consumer group will maintain its new set of offsets and receive messages independently from the consumer groups on the same topic to say it simply a message received by consumer group one will also be received by consumer group two within a single consumer group a consumer will not receive the same messages as other consumers in the same consumer group each consumer group is identified by a group ID it must be unique for each group that is two consumers that have different group IDs will not be in the same group so in our social media app you could have a consumer group dedicated to monitoring post topics that can provide valuable insights into what's trending in a social media by analyzing a portion of the post these consumers can identify trending hashtags and update a live dashboard accordingly this information can help businesses stay informed about the latest Trend and tailor their content to resonate With Their audience a separate consumer group focused on the likes topic can handle sending out real-time notifications whenever someone likes your content this feature can be particularly useful for businesses looking to engage with their audience and build relationships with potential customers by promptly acknowledging likes businesses can show their appreciation and encourage further interaction another potential consumer group could be responsible for sentiment analysis this consumers would analyze the sentiment of post and comments related to your brand or a specific topic this information can provide businesses with valuable insights into how their brand is being perceived and can help them adjust the marketing strategy accordingly after you start to read a bit more about kafa consumers and start configuring your own consumers you will see the term offset and commit more and more Apache Kafka does not have an explicit way of tracking which messages has been read by consumer of a group inad it allows consumers to track the offset that is the position in the queue of what messages it has read for a given partition in a topic to do this once in a while consumer in consumer group will publish a special message topic for the topic or partition with the committed offset for each partition it has got an up to Apache Kafka stores consumer offset in a special internal topic called consumer offsets so when a new consumer is added to a consumer group it will start consuming messages from partitions previously assigned to another consumer it will generally pick up from the most recently committed offset if a consumer leaves or crashes the partition it used to consume will be picked up by one of the consumers that is still in the consumer group this change in partitions can also occur when a topic gets modified or more partitions are added to the topic the process of changing which consumer is assigned to what partition is also called rebalancing rebalances are normal part of Apache Kafka operations and will occur during configuration changes scaling or if a broker or consumer crashes generally you want to avoid rebalance if you care about real-time consumptions of messages as some messages cannot be read during rebalance which results in loss of availability it's important to note that Kafka consumers pull data from the topic they are listening to this allows different consumers to consume messages at different rates without the broker or Kafka having to determine the D data rate for each consumer it also allows better batch processing mechanics luckily the kafa consumer API also allows client applications to treat the semantics as push if you wish for example you get a message as soon as it is ready without the need to worry about the client getting overwhelmed when it comes to delivering messages in Kafka there are few key guarantees or semantics that you you can choose from the semantics Define how Kafka ensure messages that get delivered from producers to Consumers so in our social media app for instance delivery semantics determine how reliably these likes and post get sent and processed at most once delivery prioritizes speed Above All Else messages might be delivered once but there is a possibility of losing messages entirely if there are failures the key here is that the consumers will never receive duplicates for example a user Scrolls quickly through their feed generating scen events at most once might be okay here as losing a few scene event is likely not a big deal at least once delivery is the most commonly used option in Kafka it offers a strong guarantee a message will be delivered one or more times to its consumers this means you can be sure your messages won't be lost entirely but there's a chance you might see duplicates exactly once delivery strives for a perfect balance each message is guaranteed to be delivered only once to each consumer this is ideal for scenarios where duplicates are absolutely critical like Financial transactions however achieving exactly one semantics requires more complex configurations and can introduce some performance overhead imagine a user credits your social media app with $1 to buy coins for an inapp purchase exactly once delivery would ensure this critical financial transaction is only recorded once in your system preventing duplicate charges the best delivery semantics for your application depend on specific needs if you can tolerate duplicates and priortize speed and throughput at least once is a strong option if ensuring zero duplicates is vital exactly once delivery may be the way to go even if it means some trade-offs in performance earlier we learned that a kafa cluster isn't just one machine it's a powerful distributed system made by multiple servers called Brokers these brokers work together to provide a highly available and scalable platform for handling real-time data streams each broker stores a portion of their data and is responsible for processing request from producers and consumers they receive data from producers and send it out to Consumers Brokers act as the receiving point for messages from producers ensuring they are safely stored and replicated for fall tolerance Brokers efficiently store messages published by The Producers within the topic partitions assigned to them br Focus manage consumer subscriptions and deliver messages to the appropriate consumers based on their subscriptions and offsets and for reliability Kafka replicates each partition across multiple brokers in the cluster this means there are copies of the data on different servers if a broker fails another broker with a replica can take over and continue serving messages ensuring minimal disruptions to data access imagine a large library with many books a single librarian managing all these books would be overwhelmed similarly a single broker wouldn't be able to handle a massive data efficiently that is why Kafka topics are divided into smaller more manageable units called partitions these partitions are then distributed across the brokers in your cluster this distribution strategy has two main benefits scalability and parallel processing so if your data keeps growing you can simply add more Brokers to the cluster each new broker can take on some of the data storage and processing load keeping your system running smoothly consumers can read data from multiple partitions simultaneously across different Brokers this parallelism allows Kafka to process large volumes of data much faster than if everything was stored on a single server one of kafka's key strength is the horizontal scalability it enables you to add Brokers over time to your kafa cluster allowing you to scale to hundreds of Brokers and handle massive amounts of messages throughputs of millions of messages per second the scalability makes it well suited for large scale applications such as Twitter where real-time data processing is critical the image here illustrates the basic building blocks of how kafa stores and manages streams of data ready for sending and receiving by applications it illustrates their relationships between topics consumers partitions and other Concepts like replicas Brokers producers Etc take a moment pause the video to revise and understand their relationships better Apache Kafka applications are as diverse as the organizations that use it it's a highly skillable messaging system for distributed systems facilitating realtime analytics metric collection log aggregation and stream processing ca's popularity is evident with over 2,000 firms and 80% of the Fortune 100 companies using it Netflix relies on Kafka to power it realtime recommendation engine delivering personalized moving and TV show suggestions to users Uber leverages Kafka to track user activities and provide real-time ride updates ensuring a smooth and seamless ride experience LinkedIn collects an analyzes user interactions through Kafka to gain valuable insights into its member Behavior arbnb processes reservation data and manages property Lance using kfka enabling efficient booking and Property Management while Walmart monitors inventory levels and optimizes at supply chain with the help of Kafka ensuring efficient Inventory management and customer satisfaction in essence Apache Kafka acts as the backbone of modern data integration enabling organizations to unlock the full potential of their data Drive Innovation and Achieve business success with that we have covered the foundation of Kafka and if you ready to take it to the next level get set for a deep dive in my next video [Music]